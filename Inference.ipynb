{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPofw7Z3bIiyBZlEgy9OXrA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 모델 학습\n"],"metadata":{"id":"mq8HneYolFTI"}},{"cell_type":"code","source":["# 학습을 위해서 train, val 섞어서 나누기\n","%cp train.txt train_full.txt\n","%cp val.txt   val_full.txt\n","\n","# 무작위 섞기\n","!shuf train_full.txt > train_shuf.txt\n","!shuf val_full.txt   > val_shuf.txt\n","\n","# train을 2,000줄씩 샤딩 (train_shard_00.txt, train_shard_01.txt, ...)\n","\n","!split -l 2000 -d -a 2 train_shuf.txt train_shard_\n","!for f in train_shard_*; do mv \"$f\" \"${f}.txt\"; done\n","\n","# val은 500줄\n","!head -n 500 val_shuf.txt > val_small.txt\n","\n","%cd /content/drive/MyDrive/tacotron2\n","\n","from glob import glob\n","\n","shards = sorted(glob(\"/content/drive/MyDrive/tacotron2/filelists/train_shard_*.txt\"))\n","for i,f in enumerate(shards):\n","    print(f\">>> training on {f}\")\n","\n","    !python /content/drive/MyDrive/tacotron2/train.py \\\n","      --output_directory /content/drive/MyDrive/tacotron2/output/korean-jamo \\\n","      --log_directory /content/drive/MyDrive/tacotron2/logs \\\n","      --checkpoint_path /content/drive/MyDrive/tacotron2/tacotron2_statedict_gpu.pt \\\n","      --warm_start \\\n","      --hparams \"training_files=$f,validation_files=/content/drive/MyDrive/tacotron2/filelists/val_small.txt,epochs=300,iters_per_checkpoint=2000\"\n","# !python /content/drive/MyDrive/tacotron2/train.py \\\n","#   --output_directory /content/drive/MyDrive/tacotron2/output/korean-roman \\\n","#   --log_directory /content/drive/MyDrive/tacotron2/logs \\\n","#   --checkpoint_path /content/drive/MyDrive/tacotron2/tacotron2_statedict_gpu.pt \\\n","#   --warm_start \\\n","#   --hparams \"training_files=/content/drive/MyDrive/tacotron2/filelists/train.txt,validation_files=/content/drive/MyDrive/tacotron2/filelists/val.txt,epochs=10000\""],"metadata":{"id":"tziqok2fljZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import librosa\n","import soundfile as sf\n","import os\n","from glob import glob\n","\n","# 훈련 모델에서 사용하는 음성 주파수와 데이터셋의 주파수를 맞추는 전처리\n","# 22050 으로 변경하는 로직\n","\n","TARGET_DIR = 'kss/'  # KSS 데이터셋의 .wav 파일이 있는 경로\n","TARGET_SR = 22050\n","ORIGINAL_SR = 44100\n","# -----------------\n","\n","# 모든 wav 파일 경로 찾기 (재귀적으로 검색)\n","wav_files = glob(os.path.join(TARGET_DIR, '**', '*.wav'), recursive=True)\n","\n","print(f\"총 {len(wav_files)}개의 파일을 22050 Hz로 변환하여 덮어씌웁니다.\")\n","print(\"원본 파일 백업 여부를 확인하세요!\")\n","\n","for i, file_path in enumerate(wav_files):\n","    try:\n","        # 1. 오디오 파일 로드\n","        y, sr = librosa.load(file_path, sr=ORIGINAL_SR)\n","\n","        # 2. 리샘플링 수행 (44100 Hz -> 22050 Hz)\n","        y_resampled = librosa.resample(y, orig_sr=ORIGINAL_SR, target_sr=TARGET_SR)\n","\n","        # 3. 리샘플링된 파일 저장 (원본 파일 덮어쓰기)\n","        # sf.write는 기존 파일을 삭제하고 새 파일로 대체합니다.\n","        sf.write(file_path, y_resampled, TARGET_SR, format='WAV', subtype='PCM_16')\n","\n","        if (i + 1) % 100 == 0:\n","            print(f\"--- {i + 1}개 파일 변환 완료: {file_path} ---\")\n","\n","    except Exception as e:\n","        print(f\"오류 발생: {file_path}. 오류 내용: {e}\")\n","        continue\n","\n","print(\"모든 파일 변환 및 덮어쓰기가 완료되었습니다.\")"],"metadata":{"id":"9ufnzrEilIOL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 학습한 모델을 가지고 추론을 할 경우"],"metadata":{"id":"6MULC76EkjoZ"}},{"cell_type":"code","source":["import matplotlib.pylab as plt\n","\n","def plot_data(data, figsize=(16, 4)):\n","    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n","    if len(data) == 1:\n","        axes = [axes]  # 단일 축 대응\n","    for i in range(len(data)):\n","        axes[i].imshow(\n","            data[i],\n","            aspect='auto',\n","            origin='lower',    # ← 'bottom' 대신 'lower'\n","            interpolation='none'\n","        )\n","    plt.tight_layout()\n","    return fig"],"metadata":{"id":"8tTvaYvkkbt_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","waveglow = torch.hub.load(\n","    'NVIDIA/DeepLearningExamples:torchhub',\n","    'nvidia_waveglow',\n","    model_math='fp32',     # 또는 'fp16' (fp16 쓰면 mel/모델도 half로 맞추세요)\n","    pretrained=True,\n","    trust_repo=True,\n","    force_reload=False,\n",").to(device).eval()"],"metadata":{"id":"sHMXV3Iiki0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/tacotron2\n","\n","import torch\n","from train import create_hparams\n","from train import load_model\n","from text import text_to_sequence\n","\n","def test(text, file, num):\n","  hparams = create_hparams(\"distributed_run=False\")\n","\n","  tacotron2 = load_model(hparams).to(\"cuda:0\").eval()\n","  ckpt_path = file  # 실제 파일명\n","  try:\n","    ckpt = torch.load(ckpt_path, map_location=\"cuda:0\", weights_only=True)\n","  except Exception:\n","      ckpt = torch.load(ckpt_path, map_location=\"cuda:0\", weights_only=False)\n","\n","  sd = ckpt.get('state_dict',ckpt)\n","  tacotron2.load_state_dict(sd,strict=False)\n","\n","  seq = torch.LongTensor(\n","      text_to_sequence(text, hparams.text_cleaners)\n","  ).unsqueeze(0).to(\"cuda:0\")\n","  import time\n","\n","  start = time.perf_counter()\n","  with torch.no_grad():\n","    mel_outputs, mel_postnet, _, alignments = tacotron2.inference(seq)\n","  end = time.perf_counter()\n","  elapsed = end - start\n","\n","  print(str(elapsed) + \"초 걸림\")\n","\n","  mel = mel_postnet.squeeze(0).cpu()\n","  mel = mel_postnet.squeeze(0).cpu()\n","\n","  fig = plot_data((mel_outputs.float().data.cpu().numpy()[0],\n","          mel_postnet.float().data.cpu().numpy()[0],\n","          alignments.float().data.cpu().numpy()[0].T))\n","\n","  fig.savefig(\"/content/\" + file.split(\"/\")[-1] + \"-\" + num + \"-out.png\")\n","  plt.close(fig)\n","\n","  with torch.no_grad():\n","    audio = waveglow.infer(mel_outputs, sigma=0.7)          # audio: [1, n_samples]\n","    audio = denoiser(audio, strength=0.01)[:, 0, :] # 옵션: 약한 잔향 제거\n","    audio = audio.squeeze(0).float().cpu().numpy()\n","\n","  # 3) 저장\n","  import soundfile as sf\n","  sf.write( \"/content/\" + file.split(\"/\")[-1] + \"-\" + num +  '.wav', audio, samplerate=22050)"],"metadata":{"id":"5qQ0vYO_kOW5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# WER/CER 로 에러 비교\n"],"metadata":{"id":"S2LmqKVYk5t8"}},{"cell_type":"code","source":["import whisper\n","from jiwer import wer\n","from jiwer import wer, transforms as tr\n","from jiwer import cer\n","\n","\n","def t(audio_path):\n","  transform = tr.Compose([\n","    tr.Strip(),                # 앞뒤 공백 제거\n","    tr.ToLowerCase(),          # 소문자\n","    tr.RemoveMultipleSpaces(), # 여러 공백 -> 하나\n","  ])\n","\n","# 1) Whisper 모델 로드 (용량 따라 tiny/small/medium 등 선택)\n","  model = whisper.load_model(\"small\")  # \"base\", \"small\", \"medium\" 등 가능\n","  ref_text = \"간장 공장 공장장은 강 공장장이고 된장 공장 공장장은 장 공장장이다.\"\n","\n","  # 3) 음성 → 텍스트 (한국어라면 language=\"ko\" 권장)\n","  result = model.transcribe(audio_path, language=\"ko\")\n","  hyp_text = result[\"text\"].strip()\n","  ref_norm = transform(ref_text)\n","  hyp_norm = transform(hyp_text)\n","  # 각 샘플 WER\n","  sample_wer = wer(ref_norm, hyp_norm)\n","  sample_cer = cer(ref_norm, hyp_norm)\n","  print(f\"[{audio_path}]\")\n","  print(f\"  원본: {ref_text}\")\n","  print(f\"  음성: {hyp_text}\")\n","  print(f\"  WER: {sample_wer:.3f}\\n\")\n","  print(f\"  CER: {sample_cer:.3f}\\n\")"],"metadata":{"id":"FIlf5VHLkp-y"},"execution_count":null,"outputs":[]}]}